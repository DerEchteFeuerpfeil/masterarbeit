stage1_params:
  name: "VSQ"
  vector_decoder_model: "mlp"
  quantized_dim: 512
  codebook_size: 4096  # will be ignored for FSQ
  image_loss: "mse"
  vq_method: "fsq"
  fsq_levels: [7,5,5,5,5]
  num_segments: 3
  num_codes_per_shape: 2
  pred_color: false
  lseg: 8.0
  checkpoint_path: "results/VSQ_l/checkpoints/last.ckpt"

model_params:
  name: "VQ_SVG_Stage2"
  max_seq_len: 562  # 512 + 50 for text
  dim: 512
  ff_mult: 2
  depth: 16
  heads: 8
  text_encoder_str: "google/bert_uncased_L-12_H-512_A-8"
  use_alibi_positional_bias : False

data_params:
  dataset: "figr8"
  csv_path: ".data/stage2_split.csv"  # root path for CausalSVGDataModule in dataset.py
  vq_token_npy_path: ".data/tokenized.npy"
  grid_size: 128
  train_batch_size: 8
  val_batch_size:  8
  num_workers: 32
  width: 128
  min_context_length: 10  # all samples below this will be removed from the dataset
  fraction_of_class_only_inputs: 0.0  # fraction of samples that will only have the "class" entry of the dataframe as input
  fraction_of_blank_inputs: 0.1  # fraction of samples that will have empty text input
  fraction_of_iconshop_chatgpt_inputs: 0.3
  fraction_of_full_description_inputs: 0.6
  shuffle_vq_order: False  # whether to shuffle the order of the VQ tokens, its not really "shuffling", but more cutting the sequence into two parts and switching their order
  use_pre_computed_text_tokens_only: False  # if True, the text tokens that were pre-computed will be used as input, if False, the text will be tokenized in the dataloader (according to the specified fractions).

exp_params:
  lr: 0.0006
  weight_decay: 1.e-4  # specify positive float to enable, start experimenting with 1.e-4/1.e-3
  scheduler_gamma: 0.96  # 0.95 is a good starting value
  warmup_steps: 50000
  train_log_interval: 0.25  # decides how often full generations are made and logged
  val_log_interval: 0.5
  # metric_log_interval: 1.0
  manual_seed: 42
  post_process: False  # whether to log with svg fixing
  # continue_checkpoint: ""

trainer_params:
  devices: -1  # always keep at -1 as this takes all available GPUs specified through CUDA_VISIBLE_DEVICES
  max_epochs: 100  # dsnt matter too much, got early stopping implemented
  accumulate_grad_batches: 4

logging_params:
  entity: ""  # wandb entity
  project: ""  # your wandb project name
  save_dir: "results/TM_l"
  name: "Stage 2 Figr8 TM_l"  # name of the run in wandb
  version: 0
  author: "Moritz"  # will be a tag in wandb
  # id: ""