{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from utils import svg_string_to_tensor, svg_to_tensor\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import yaml\n",
    "from models import VSQ, VQ_SVG_Stage2\n",
    "from utils import get_side_by_side_reconstruction, map_wand_config\n",
    "from dataset import VSQDataset, VQDataset, VQDataModule\n",
    "from tokenizer import VQTokenizer\n",
    "from svg_fixing import min_dist_fix_global, min_dist_fix\n",
    "from glob import glob\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.decoder import SketchDecoder\n",
    "from deepsvg.difflib.tensor import SVGTensor\n",
    "from deepsvg.svglib.svg import SVG\n",
    "from deepsvg.svglib.geom import Bbox\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# im2vec\n",
    "from dataset import GenericRasterizedSVGDataset\n",
    "from models.vector_vae_nlayers import VectorVAEnLayers\n",
    "import pydiffvg\n",
    "from typing import List\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model_from_basepath(basepath, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    returns model, ds, config\n",
    "    \"\"\"\n",
    "    config = yaml.load(open(os.path.join(basepath, 'config.yaml'), 'r'), Loader=yaml.FullLoader)\n",
    "    config[\"data_params\"][\"max_shapes_per_svg\"] = 2000\n",
    "    config[\"data_params\"][\"train_batch_size\"] = 2\n",
    "    config[\"data_params\"][\"val_batch_size\"] = 2\n",
    "    model = VSQ(**config[\"model_params\"]).to(device)\n",
    "    all_ckpts = glob(os.path.join(basepath, \"checkpoints\", \"*.ckpt\"))\n",
    "    # sort by date\n",
    "    latest_ckpt_path = sorted(all_ckpts, key=os.path.getmtime)[-1]\n",
    "    state_dict = torch.load(latest_ckpt_path, map_location=device)[\"state_dict\"]\n",
    "    try:\n",
    "        model.load_state_dict(state_dict)\n",
    "    except:\n",
    "        model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in state_dict.items()})\n",
    "    ds = VSQDataset(**config[\"data_params\"], train=False)\n",
    "    model = model.eval()\n",
    "    return model, ds, config\n",
    "\n",
    "def load_stage2_model_from_basepath(vsq_model, basepath, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    returns model, ds, config\n",
    "    \"\"\"\n",
    "    config = yaml.load(open(os.path.join(basepath, 'config.yaml'), 'r'), Loader=yaml.FullLoader)\n",
    "    config = map_wand_config(config)\n",
    "    config[\"data_params\"][\"train_batch_size\"] = 2\n",
    "    config[\"data_params\"][\"val_batch_size\"] = 2\n",
    "\n",
    "    tokenizer = VQTokenizer(vsq_model, \n",
    "                        config[\"data_params\"].get(\"grid_size\") or config[\"data_params\"].get(\"width\"), \n",
    "                        config['stage1_params'][\"num_codes_per_shape\"], \n",
    "                        config[\"model_params\"][\"text_encoder_str\"],\n",
    "                        lseg = config[\"stage1_params\"][\"lseg\"], \n",
    "                        device = device,\n",
    "                        max_text_token_length=config[\"data_params\"].get(\"max_text_token_length\") or 50)\n",
    "\n",
    "    model = VQ_SVG_Stage2(tokenizer, **config[\"model_params\"], device=device)\n",
    "\n",
    "    text_only_tokenizer = VQTokenizer(vsq_model, \n",
    "                                  config[\"data_params\"].get(\"grid_size\") or config[\"data_params\"].get(\"width\"), \n",
    "                                  config['stage1_params'][\"num_codes_per_shape\"], \n",
    "                                  config[\"model_params\"][\"text_encoder_str\"], \n",
    "                                  use_text_encoder_only=True, \n",
    "                                  lseg=config[\"stage1_params\"][\"lseg\"],\n",
    "                                  codebook_size=tokenizer.codebook_size,\n",
    "                                  max_text_token_length=config[\"data_params\"].get(\"max_text_token_length\") or 50,)\n",
    "    dm = VQDataModule(tokenizer=text_only_tokenizer,\n",
    "                    **config[\"data_params\"], \n",
    "                    context_length=config['model_params']['max_seq_len'],\n",
    "                    train=False)\n",
    "    dm.setup()\n",
    "    \n",
    "    all_ckpts = glob(os.path.join(basepath, \"checkpoints\", \"*.ckpt\"))\n",
    "    all_ckpts = [ckpt for ckpt in all_ckpts if not \"last\" in ckpt]\n",
    "    # sort by date\n",
    "    latest_ckpt_path = sorted(all_ckpts, key=os.path.getmtime)[-1]\n",
    "    print(f\"loading from {latest_ckpt_path}\")\n",
    "    state_dict = torch.load(latest_ckpt_path, map_location=device)[\"state_dict\"]\n",
    "    try:\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"loaded weights successfully\")\n",
    "    except:\n",
    "        model.load_state_dict({k.replace(\"model.\", \"\", 1) if k.startswith(\"model.\") else k: v for k, v in state_dict.items()})\n",
    "        print(\"loaded weights successfully\")\n",
    "    model = model.eval()\n",
    "    return model, dm, config\n",
    "\n",
    "def raster_svg(pixels: np.ndarray):\n",
    "    # deepcopy\n",
    "    pixels = pixels.copy()\n",
    "    try:\n",
    "        pixels -= 6  # 3 END_TOKEN + 1 SVG_END + 2 CAUSAL_TOKEN\n",
    "\n",
    "        svg_tensors = []\n",
    "        path_tensor = []\n",
    "        for i, pix in enumerate(pixels):\n",
    "            # COMMAND = 0\n",
    "            # START_POS = [1, 3)\n",
    "            # CONTROL1 = [3, 5)\n",
    "            # CONTROL2 = [5, 7)\n",
    "            # END_POS = [7, 9)\n",
    "            if pix[0] == -3:  # Move\n",
    "                cmd_tensor = np.zeros(9)\n",
    "                cmd_tensor[0] = 0\n",
    "                cmd_tensor[7:9] = pixels[i+2]\n",
    "                start_pos = pixels[i+1]\n",
    "                end_pos = pixels[i+2]\n",
    "                if np.all(start_pos == end_pos) and path_tensor:\n",
    "                    svg_tensors.append(torch.tensor(path_tensor))\n",
    "                    path_tensor = []\n",
    "                path_tensor.append(cmd_tensor.tolist())\n",
    "            elif pix[0] == -2:  # Line\n",
    "                cmd_tensor = np.zeros(9)\n",
    "                cmd_tensor[0] = 1\n",
    "                cmd_tensor[7:9] = pixels[i+1]\n",
    "                path_tensor.append(cmd_tensor.tolist())\n",
    "            elif pix[0] == -1:  # Curve\n",
    "                cmd_tensor = np.zeros(9)\n",
    "                cmd_tensor[0] = 2\n",
    "                cmd_tensor[3:5] = pixels[i+1]\n",
    "                cmd_tensor[5:7] = pixels[i+2]\n",
    "                cmd_tensor[7:9] = pixels[i+3]\n",
    "                path_tensor.append(cmd_tensor.tolist())\n",
    "        append_t = torch.tensor(path_tensor)\n",
    "        if append_t.size(0) > 0:\n",
    "            svg_tensors.append(append_t)\n",
    "        return [svg_tensors]\n",
    "    except Exception as error_msg:  \n",
    "        print(error_msg, pixels)\n",
    "        assert False, \"error in raster_svg\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def iconshop_gen_to_svg(sample, bbox=200):\n",
    "    gen_data = []\n",
    "    svgs = []\n",
    "\n",
    "    for sample_pixel in sample:\n",
    "        gen_data += raster_svg(sample_pixel)\n",
    "\n",
    "    for index, data in enumerate(gen_data):\n",
    "        print(\"decoding svg\", index)\n",
    "        paths = []\n",
    "\n",
    "        for d in data:\n",
    "            path = SVGTensor.from_data(d)\n",
    "            path = SVG.from_tensor(path.data, viewbox=Bbox(bbox))\n",
    "            path.fill_(True)\n",
    "            paths.append(path)\n",
    "        path_groups = paths[0].svg_path_groups\n",
    "        for k in range(1, len(paths)):\n",
    "            path_groups.extend(paths[k].svg_path_groups)\n",
    "        svg = SVG(path_groups, viewbox=Bbox(bbox))\n",
    "        svgs.append(svg)\n",
    "\n",
    "    return svgs\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_iconshop_from_text(text:str, model, tokenizer, n_samples=1) -> list[SVG]:\n",
    "    encoded_dict = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=50,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,  # for RoBERTa\n",
    "        )\n",
    "    batched_text_tokens = torch.stack([encoded_dict[\"input_ids\"].squeeze()]).to(device)\n",
    "    sample = model.sample(n_samples=n_samples, text=batched_text_tokens.repeat(n_samples, 1))\n",
    "    return iconshop_gen_to_svg(sample)\n",
    "\n",
    "def pretty_print_trainable_params(model, only_trainable:bool=False):\n",
    "    print(f\"Num trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")\n",
    "    if not only_trainable:\n",
    "        print(f\"Num params: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "def map_wand_config(config):\n",
    "    new_config = {}\n",
    "    for k, v in config.items():\n",
    "        if not \"wandb\" in k:\n",
    "            if \"value\" in v and isinstance(v, dict):\n",
    "                new_config[k] = v[\"value\"]\n",
    "            else:\n",
    "                new_config[k] = v\n",
    "    return new_config\n",
    "\n",
    "def save_im2vec_points_to_svg(model:VectorVAEnLayers,\n",
    "                            all_points:List, \n",
    "                            imsize, \n",
    "                            save_base_dir, \n",
    "                            filename):\n",
    "    shape_groups = []\n",
    "    shapes = []\n",
    "    for k in range(len(all_points)):\n",
    "        # Get point parameters from network\n",
    "        points = all_points[k].cpu()#[self.sort_idx[k]]\n",
    "        if points.ndim > 2:\n",
    "            points = points.squeeze(0)\n",
    "        points = points * imsize\n",
    "        color = torch.cat([torch.tensor([0,0,0,1]),])\n",
    "        num_ctrl_pts = torch.zeros(model.curves, dtype=torch.int32) + 2\n",
    "\n",
    "        path = pydiffvg.Path(\n",
    "            num_control_points=num_ctrl_pts, points=points,\n",
    "            is_closed=True)\n",
    "\n",
    "        shapes.append(path)\n",
    "        path_group = pydiffvg.ShapeGroup(\n",
    "            shape_ids=torch.tensor([len(shapes) - 1]),\n",
    "            fill_color=None,\n",
    "            stroke_color=color)\n",
    "        shape_groups.append(path_group)\n",
    "    pydiffvg.save_svg(f\"{save_base_dir}/{filename}\",\n",
    "                        imsize, imsize, shapes, shape_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference IconShop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLE = 1\n",
    "BS = 4\n",
    "BBOX = 200\n",
    "cfg = {\n",
    "    'pix_len': 512,\n",
    "    'text_len': 50,\n",
    "\n",
    "    'tokenizer_name': 'google/bert_uncased_L-12_H-512_A-8',\n",
    "    'word_emb_path': 'iconshop_checkpoints/word_embedding_512.pt',\n",
    "    'pos_emb_path': None,\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.read_csv(\".data/stage2_split.csv\")\n",
    "df = df[df[\"split\"] == \"test\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg['tokenizer_name'])\n",
    "\n",
    "sketch_decoder = SketchDecoder(\n",
    "    config={\n",
    "        'hidden_dim': 1024,\n",
    "        'embed_dim': 512, \n",
    "        'num_layers': 16, \n",
    "        'num_heads': 8,\n",
    "        'dropout_rate': 0.1  \n",
    "    },\n",
    "    pix_len=cfg['pix_len'],\n",
    "    text_len=cfg['text_len'],\n",
    "    num_text_token=tokenizer.vocab_size,\n",
    "    word_emb_path=cfg['word_emb_path'],\n",
    "    pos_emb_path=cfg['pos_emb_path'],\n",
    ")\n",
    "sketch_decoder.load_state_dict(torch.load(os.path.join(\"iconshop_checkpoints\",\"epoch_100\", 'pytorch_model.bin')))\n",
    "sketch_decoder = sketch_decoder.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=1\n",
    "text = \"heart heart-shape hearts like love\"\n",
    "all_svgs = sample_iconshop_from_text(text, sketch_decoder, tokenizer, n_samples=n_samples)\n",
    "\n",
    "all_svgs[0].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset text prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_svgs = []\n",
    "all_texts = []\n",
    "for i in range(25):\n",
    "    row = df.sample(1)\n",
    "    text = row.description.values[0]\n",
    "    svgs = sample_iconshop_from_text(text, sketch_decoder, tokenizer, n_samples=1)\n",
    "    all_svgs.extend(svgs)\n",
    "    all_texts.extend([text] * len(svgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_svgs)):\n",
    "    print(all_texts[i])\n",
    "    all_svgs[i].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference $VSQ_l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"results/VSQ_l\"\n",
    "model, ds, config = load_model_from_basepath(base_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import drawing_to_tensor\n",
    "\n",
    "drawings = []\n",
    "grid_sizes = [56, 72, 100, 128, 200, 256]\n",
    "idx = 3\n",
    "\n",
    "for grid_size in grid_sizes:\n",
    "    img, drawing = get_side_by_side_reconstruction(model, \n",
    "                                                   ds, \n",
    "                                                   idx = idx, \n",
    "                                                   device = device, \n",
    "                                                   dataset_name=\"glyphazzn\", \n",
    "                                                   override_global_stroke_width=0.4, \n",
    "                                                   return_drawing=True, \n",
    "                                                   quantize_grid_size=grid_size)\n",
    "    drawings.append(drawing)\n",
    "\n",
    "\n",
    "plt.imshow(make_grid([drawing_to_tensor(drawings[0]), drawing_to_tensor(drawings[-1])]).permute(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference $TM_l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset returns: text_tokens, attention_mask, vq_tokens, vq_targets, torch.ones(1).to(text_tokens.device)*self.pad_token\n",
    "stage2_base_path = \"results/TM_l\"\n",
    "\n",
    "# load config to extract stage1 params\n",
    "config = yaml.load(open(os.path.join(stage2_base_path, 'config.yaml'), 'r'), Loader=yaml.FullLoader)\n",
    "config = map_wand_config(config)\n",
    "\n",
    "# load VSQ\n",
    "vsq_base_path = config[\"stage1_params\"][\"checkpoint_path\"].split(\"checkpoints\")[0]\n",
    "vsq_model = load_model_from_basepath(vsq_base_path, device=device)[0]\n",
    "\n",
    "stage2_model, stage2_dm, stage2_config = load_stage2_model_from_basepath(vsq_model, stage2_base_path, device=device)\n",
    "stage2_val_dl = stage2_dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_model._generate_from_text(\"camera picture\", temperature=0.0, return_drawing=True, post_process=True, global_position_fixing=True, max_dist = 2.0, v2=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_drawings = []\n",
    "prompts = []\n",
    "generations = []\n",
    "stage2_ds = stage2_val_dl.dataset\n",
    "\n",
    "for i in range(20):\n",
    "    random_idx = np.random.randint(0, len(stage2_ds))\n",
    "    text_tokens, attention_mask, vq_tokens, vq_targets, pad_tokens = stage2_ds[random_idx]\n",
    "\n",
    "    text = stage2_model.tokenizer.decode_text(text_tokens)\n",
    "    prompts.append(text)\n",
    "    print(f\"Doing '{text}' - {i+1}/{20}\")\n",
    "\n",
    "    vq_tokens = torch.tensor([1], device=device, dtype=torch.int64)\n",
    "    text_tokens = text_tokens.unsqueeze(0).to(device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "    vq_tokens = vq_tokens.unsqueeze(0).to(device)\n",
    "\n",
    "    generation, reason = stage2_model.generate(text_tokens, attention_mask, vq_tokens, temperature=0.1,sampling_method=\"top_p\", sampling_kwargs={\"thres\":0.5})\n",
    "    generations.append(generation)\n",
    "    gen_drawing = stage2_model.tokenizer._tokens_to_svg_drawing(generation, post_process=True, max_dist_frac=0.01)\n",
    "    generation_drawings.append(gen_drawing)\n",
    "\n",
    "print(prompts[0])\n",
    "generation_drawings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Im2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup & load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "im2vecsweep_base_config = {\n",
    "    \"base_path\": \"results/Im2Vec\",\n",
    "    \"im2vec_model_path\": \"checkpoints/last-v2.ckpt\",\n",
    "    \"im2vec_config_path\": \"config.yaml\",\n",
    "    \"out_base_dir\": \"results/Im2Vec\",\n",
    "    \"dataset\": \"icons\",\n",
    "}\n",
    "\n",
    "class_name = \"figr8_full\"\n",
    "\n",
    "selected_config = im2vecsweep_base_config\n",
    "im2vecsweep_base_config[\"class_name\"] = class_name\n",
    "\n",
    "base_path = os.path.join(selected_config[\"base_path\"], class_name)\n",
    "im2vec_model_path = os.path.join(base_path, selected_config[\"im2vec_model_path\"])\n",
    "im2vec_config_path = os.path.join(base_path, selected_config[\"im2vec_config_path\"])\n",
    "dataset = selected_config[\"dataset\"]\n",
    "out_base_dir = os.path.join(selected_config[\"out_base_dir\"], class_name)\n",
    "\n",
    "with open(im2vec_config_path, \"r\") as f:\n",
    "        try:\n",
    "            im2vec_config = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "im2vec_config = map_wand_config(im2vec_config)\n",
    "\n",
    "im2vec_config[\"model_params\"][\"imsize\"] = 128\n",
    "im2vec_config[\"model_params\"][\"latent_dim\"] = 256\n",
    "im2vec_config[\"data_params\"][\"img_size\"] = 128\n",
    "\n",
    "\n",
    "# ds = GenericRasterizedSVGDataset(**im2vec_config[\"data_params\"], train=None)\n",
    "im2vec = VectorVAEnLayers(**im2vec_config[\"model_params\"])\n",
    "state_dict = torch.load(im2vec_model_path, map_location=device)[\"state_dict\"]\n",
    "try:\n",
    "    im2vec.load_state_dict(state_dict)\n",
    "except:\n",
    "    im2vec.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in state_dict.items()})\n",
    "\n",
    "im2vec = im2vec.eval().to(device)\n",
    "im2vec.base_control_features = im2vec.base_control_features.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference (no conditioning as Im2Vec is just a VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_points = im2vec.multishape_sample(10, return_points=True, device=device)\n",
    "idx = 0\n",
    "save_im2vec_points_to_svg(im2vec, samples_points[idx], 72, \"results/Im2Vec\",f\"im2vec_sample_{idx}.svg\")\n",
    "\n",
    "# adjust these settings for altering visual appearance\n",
    "img = svg_to_tensor(f\"im2vec_sample_{idx}.svg\", new_stroke_color=\"black\", new_fill_color=\"none\", new_stroke_width=0.7, output_width=480)\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
